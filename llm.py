from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from fewshot_doc import answer_examples

chat_storage = {}
    
def get_llm():
    llm = ChatUpstage()
    return llm
    
# ì‚¬ì „ì„ ì°¸ê³ í•˜ì—¬ ìœ ì €ì˜ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ ì„¤ì •
def get_dictionary_chain():
    
    llm = get_llm()
    
    dictionary = [
        "ë² ì´ìŠ¤ì— ìˆëŠ” ì„ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ì£¼ì",
        "íƒ€ì„ì— ìˆëŠ” ì„ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> íƒ€ì",
        "ë§ˆìš´ë“œ ìœ„ì—ì„œ ê³µì„ ë˜ì§€ëŠ” ì„ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> íˆ¬ìˆ˜"
    ]
    
    dictionary_prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í™•ì¸í•˜ê³  ì‚¬ì „ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´ ë³€ê²½í•˜ì§€ ì•Šê³  ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë¦¬í„´í•´ì£¼ì„¸ìš”.
        ì§ˆë¬¸ì´ë‚˜ ì£¼ì–´ê°€ ëª¨í˜¸í•  ê²½ìš° history retrieverì—ì„œ history chatì„ ì°¸ê³ í•˜ì—¬ ìœ ì¶”í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë¦¬í„´í•´ì£¼ì„¸ìš”.
        ì‚¬ì „: {dictionary}
        ì§ˆë¬¸: {{question}}
                                                         """)
    
    dictionary_chain = dictionary_prompt | llm | StrOutputParser()
    return dictionary_chain
    
# ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ retriever ì„¤ì •
def get_retriever():
    
    index_name = "baseball-rules-index"

    # ë°ì´í„°ë¥¼ ë²¡í„°í™”í•  ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embedding = UpstageEmbeddings(model="solar-embedding-1-large")
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)
    retriever = database.as_retriever(search_kwargs={"k": 4})
    return retriever
    
# chat historyë¥¼ ë°˜ì˜í•˜ì—¬ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ ì„¤ì •
def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()
    
    # chat historyë¥¼ ë°˜ì˜í•˜ì—¬ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    
    # ì±„íŒ… ê¸°ë¡ì„ ë°˜ì˜í•˜ì—¬ QAë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"), # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
            ("human", "{input}")
        ]
    )
    
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    return history_aware_retriever
    
# ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ í•¨ìˆ˜ ì„¤ì •
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ í•´ë‹¹ ì„¸ì…˜ì— ëŒ€í•œ ì±„íŒ…ê¸°ë¡ ìƒì„±
    if session_id not in chat_storage:
        chat_storage[session_id] = ChatMessageHistory()
    return chat_storage[session_id]
    
# RAG ì²´ì¸ì„ ìƒì„±
def get_rag_chain():
    
    llm = get_llm()
    
    # few-shotì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}")
        ]
    )
    
    # few-shotì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples
    )
    
    # ëª¨ë¸ì˜ ì—­í• ì„ ì •ì˜í•´ì£¼ê¸° ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = ("""
            ë‹¹ì‹ ì€ ìµœê³ ì˜ KBO ë¦¬ê·¸ ì•¼êµ¬ ê·œì¹™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ì•„ë˜ì˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
            
            ë¨„ì•½ì— ì•¼êµ¬ ê·œì¹™ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ "ğŸš«ì•¼êµ¬ ê·œì¹™ì— ëŒ€í•œ ì§ˆë¬¸ë§Œ ë‹µë³€ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
            ë§Œì•½ì— ë‹µë³€í•  ìˆ˜ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
            
            ë‹µë³€ ì‹œì— ì¶œì²˜ì— ëŒ€í•´ì„œë„ ëª…í™•í•˜ê²Œ ë°í˜€ì£¼ì‹œê³ , ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
            ë‹µë³€ì˜ ê¸¸ì´ëŠ” 2-3ì¤„ ì •ë„ë¡œ ì œí•œí•´ì£¼ì„¸ìš”.
            \n\n
            ë¬¸ì„œ: {context}
    """)
    
    # ì±„íŒ… ê¸°ë¡ ë°˜ì˜í•˜ì—¬ QAë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt, # few-shotì„ ì°¸ê³ í•˜ë„ë¡ ì „ë‹¬
            MessagesPlaceholder("chat_history"), # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸(ì±„íŒ… ê¸°ë¡)ë¥¼ ì „ë‹¬
            ("human", "{input}")
        ]
    )
    
    history_aware_retriever = get_history_retriever() # ê¸°ë¡ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ retriever
    qa_chain = create_stuff_documents_chain(llm, qa_prompt) # ëª¨ë¸ì—ê²Œ ê²€ìƒ‰í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
    rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain) # RAG ì²´ì¸ ìƒì„±
    
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain, # ì‹¤í–‰í•  RAG ì²´ì¸ì´ë‚˜ LLM ì²´ì¸
        get_session_history, # ì„¸ì…˜_ID ê¸°ë°˜ì˜ ì±„íŒ… ê¸°ë¡
        input_messages_key="input", # ìœ ì €ì˜ ì…ë ¥ì´ ë‹´ê¸´ í‚¤
        history_messages_key="chat_history", # ì±„íŒ… ê¸°ë¡ì„ ì „ë‹¬í•  í‚¤
        output_messages_key="answer" # ì¶œë ¥ ê²°ê³¼ë¥¼ ì €ì¥í•  í‚¤
    ).pick("answer")
    
    # ì±„íŒ… historyë¥¼ í¬í•¨í•œ retrieverë¥¼ í™œìš©í•˜ì—¬ rag_chain ë°˜í™˜
    return conversational_rag_chain
    
def get_ai_response(query):
    
    dictionary_chain = get_dictionary_chain()
    rag_chain = get_rag_chain()
    
    baseball_chain = {"input": dictionary_chain} | rag_chain
    
    ai_response = baseball_chain.stream(
        {"question": query},
        config={"configurable": {"session_id": "abc123"}}
    )
    
    return ai_response


# ai_response = get_ai_response()
# print(f"ai_response:\n {ai_response}\n")